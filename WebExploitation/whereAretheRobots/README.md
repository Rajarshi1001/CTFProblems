The given website link [https://jupiter.challenges.picoctf.org/problem/56830/](https://jupiter.challenges.picoctf.org/problem/56830/) looks like a very simple web page with no content. The web page has __robots.txt__ file which prevents crawler, attackers, webbots etc. from accessing the private/restricted contents of that web page. Usually the website owners want to be noticed by the search engines like __google__, __mozilla__ etc. 

The __robots.txt__ file is located at the root directory of the website pages, files etc. The _Search Engines_ indexes the website regularly using __keywords__ and __metadata__ so that it could furnish the most recent and updated contents to the users. For indexing purposes, so-called __spiders or crawlers__ are used. These are bots that the search engine companies use to fetch and index the content of all the websites that are open to them.

The spiders make a __GET__ requests to robots.txt file and if a file is found then the crawler checks for web website indentation instructions. There is only ne __robots.txt__ fel per website.

enter the url in the following search bar:

> [https://jupiter.challenges.picoctf.org/problem/56830/robots.txt](https://jupiter.challenges.picoctf.org/problem/56830/robots.txt)

Then copy the endpoint listed in the __robots.txt__ file and redirect to this following link:

> [https://jupiter.challenges.picoctf.org/problem/56830/1bb4c.html](https://jupiter.challenges.picoctf.org/problem/56830/1bb4c.html])

The corresponding flag is : __picoCTF{ca1cu1at1ng_Mach1n3s_1bb4c}__